{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28ff2ae5-49d7-40d6-893f-c554f77745f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS poc_dataengg.monitoring;\n",
    "\n",
    "CREATE OR REPLACE TABLE poc_dataengg.monitoring.data_quality_log (\n",
    "    validation_time TIMESTAMP,\n",
    "    job_name STRING,\n",
    "    trade_date DATE,\n",
    "    rule_name STRING,\n",
    "    status BOOLEAN COMMENT 'TRUE if all passed, FALSE if any failed',\n",
    "    detail STRING COMMENT 'Extra info like datatype or row count',\n",
    "    success_count BIGINT COMMENT 'Number of records that passed this rule',\n",
    "    failure_count BIGINT COMMENT 'Number of records that failed this rule',\n",
    "    bad_record STRING COMMENT 'Sample JSON of failing records (optional)'\n",
    "\n",
    ")\n",
    "USING DELTA\n",
    "PARTITIONED BY (trade_date)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81c6432c-8c72-4417-a0a8-49d5157c85db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>15</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         15
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {
             "__autoGeneratedAlias": "true"
            },
            "name": "count(1)",
            "nullable": false,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 70
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"__autoGeneratedAlias\":\"true\"}",
         "name": "count(1)",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "Select Count(*) from poc_dataengg.monitoring.data_quality_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09180ce8-5665-4bb4-a03f-bda6be3272c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "STORAGE_ACCOUNT = \"cxdlbbronze\"\n",
    "CONTAINER = \"landingzone\"\n",
    "Folder = \"BTC-USD-Partitioned\"\n",
    "\n",
    "lakehouse_path = f\"abfss://{CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net/{Folder}\"\n",
    "checkpoint_path = lakehouse_path + \"_checkpoint/last_trade.json\"\n",
    "\n",
    "client_id = dbutils.secrets.get(scope=\"landingzone-secret\", key=\"client-id\")\n",
    "client_secret = dbutils.secrets.get(scope=\"landingzone-secret\", key=\"client-secret\")\n",
    "tenant_id = dbutils.secrets.get(scope=\"landingzone-secret\", key=\"tenant-id\")\n",
    "\n",
    "account_host = f\"{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{account_host}\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{account_host}\",\n",
    "               \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{account_host}\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{account_host}\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{account_host}\",\n",
    "               f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3220e552-af80-4687-a99e-9a61709fd1b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    LongType, DecimalType, StringType,\n",
    "    TimestampType, DateType\n",
    ")\n",
    "\n",
    "expected_schema = StructType([\n",
    "    StructField(\"trade_id\", LongType(), False),                   # must always be present\n",
    "    StructField(\"trade_time\", TimestampType(), False),            # parsed from API \"time\"\n",
    "    StructField(\"trade_date\", DateType(), False),                 # derived from trade_time\n",
    "    StructField(\"price\", DecimalType(18, 2), False),              # cast from string\n",
    "    StructField(\"size\", DecimalType(18, 8), False),               # trade size with high precision\n",
    "    StructField(\"side\", StringType(), False),                     # \"buy\" / \"sell\"\n",
    "    StructField(\"ingest_ts\", TimestampType(), False)              # ingestion timestamp\n",
    "])\n",
    "\n",
    "dq_log_schema = StructType([\n",
    "    StructField(\"validation_time\", TimestampType(), False),\n",
    "    StructField(\"job_name\", StringType(), False),\n",
    "    StructField(\"trade_date\", DateType(), False),\n",
    "    StructField(\"rule_name\", StringType(), False),\n",
    "    StructField(\"status\", BooleanType(), False),\n",
    "    StructField(\"detail\", StringType(), True),\n",
    "    StructField(\"success_count\", LongType(), True),\n",
    "    StructField(\"failure_count\", LongType(), True),\n",
    "    StructField(\"bad_record\", StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8fa4a84-16a3-4dfc-bc79-bde61d310519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_incremental = spark.read.format(\"delta\").load(lakehouse_path).filter(\"trade_date='2025-09-21'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16f70c8f-06ba-4d15-b780-7e8d398733b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1098"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_incremental.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27ff8fbe-0baf-4bb8-9615-8390b2252074",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.ipykernel/1308/command-6230574895729533-178129781:12: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  dq_logs.append((datetime.utcnow(), job_name, trade_date_val,\n/root/.ipykernel/1308/command-6230574895729533-178129781:33: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  dq_logs.append((datetime.utcnow(), job_name, trade_date_val,\n/root/.ipykernel/1308/command-6230574895729533-178129781:45: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  dq_logs.append((datetime.utcnow(), job_name, trade_date_val,\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+----------------+----------+----------------------+------+-----------------+-------------+-------------+----------+\n|validation_time           |job_name        |trade_date|rule_name             |status|detail           |success_count|failure_count|bad_record|\n+--------------------------+----------------+----------+----------------------+------+-----------------+-------------+-------------+----------+\n|2025-09-21 13:26:55.087454|btc_usd_pipeline|2025-09-21|RowCount              |true  |1098 rows        |1098         |0            |NULL      |\n|2025-09-21 13:26:55.470611|btc_usd_pipeline|2025-09-21|NullCheck_trade_id    |true  |0 nulls          |1098         |0            |NULL      |\n|2025-09-21 13:26:55.774582|btc_usd_pipeline|2025-09-21|NullCheck_trade_time  |true  |0 nulls          |1098         |0            |NULL      |\n|2025-09-21 13:26:56.134044|btc_usd_pipeline|2025-09-21|NullCheck_trade_date  |true  |0 nulls          |1098         |0            |NULL      |\n|2025-09-21 13:26:56.475487|btc_usd_pipeline|2025-09-21|NullCheck_price       |true  |0 nulls          |1098         |0            |NULL      |\n|2025-09-21 13:26:56.794545|btc_usd_pipeline|2025-09-21|NullCheck_size        |true  |0 nulls          |1098         |0            |NULL      |\n|2025-09-21 13:26:57.114675|btc_usd_pipeline|2025-09-21|NullCheck_side        |true  |0 nulls          |1098         |0            |NULL      |\n|2025-09-21 13:26:57.396827|btc_usd_pipeline|2025-09-21|NullCheck_ingest_ts   |true  |0 nulls          |1098         |0            |NULL      |\n|2025-09-21 13:26:57.396972|btc_usd_pipeline|2025-09-21|SchemaCheck_trade_id  |true  |LongType()       |1098         |0            |NULL      |\n|2025-09-21 13:26:57.39699 |btc_usd_pipeline|2025-09-21|SchemaCheck_trade_time|true  |TimestampType()  |1098         |0            |NULL      |\n|2025-09-21 13:26:57.397   |btc_usd_pipeline|2025-09-21|SchemaCheck_trade_date|false |StringType()     |0            |1098         |NULL      |\n|2025-09-21 13:26:57.397011|btc_usd_pipeline|2025-09-21|SchemaCheck_price     |true  |DecimalType(18,2)|1098         |0            |NULL      |\n|2025-09-21 13:26:57.39703 |btc_usd_pipeline|2025-09-21|SchemaCheck_size      |true  |DecimalType(18,8)|1098         |0            |NULL      |\n|2025-09-21 13:26:57.397039|btc_usd_pipeline|2025-09-21|SchemaCheck_side      |true  |StringType()     |1098         |0            |NULL      |\n|2025-09-21 13:26:57.397047|btc_usd_pipeline|2025-09-21|SchemaCheck_ingest_ts |false |StringType()     |0            |1098         |NULL      |\n+--------------------------+----------------+----------+----------------------+------+-----------------+-------------+-------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date, to_json, struct\n",
    "from datetime import datetime\n",
    "\n",
    "def validate_and_log(df, expected_schema, job_name=\"btc_usd_pipeline\"):\n",
    "    dq_logs = []\n",
    "    total_count = df.count()\n",
    "\n",
    "    # Current trade_date\n",
    "    trade_date_str = df.selectExpr(\"max(trade_date)\").first()[0]; trade_date_val = datetime.strptime(trade_date_str, '%Y-%m-%d').date()\n",
    "\n",
    "    # 1. Row Count Rule\n",
    "    dq_logs.append((datetime.utcnow(), job_name, trade_date_val,\n",
    "                    \"RowCount\",\n",
    "                    total_count > 0,\n",
    "                    f\"{total_count} rows\",\n",
    "                    total_count if total_count > 0 else 0,\n",
    "                    0 if total_count > 0 else total_count,\n",
    "                    None))\n",
    "\n",
    "    # 2. Null Checks\n",
    "    for field in expected_schema.fields:\n",
    "        null_df = df.filter(col(field.name).isNull())\n",
    "        fail_count = null_df.count()\n",
    "        success_count = total_count - fail_count\n",
    "\n",
    "        bad_payload = None\n",
    "        if fail_count > 0:\n",
    "            bad_json = null_df.withColumn(\"record\", to_json(struct([col(c) for c in df.columns]))) \\\n",
    "                              .select(\"record\").limit(5)  # sample 5\n",
    "            bad_list = [r.record for r in bad_json.collect()]\n",
    "            bad_payload = \"; \".join(bad_list)\n",
    "\n",
    "        dq_logs.append((datetime.utcnow(), job_name, trade_date_val,\n",
    "                        f\"NullCheck_{field.name}\",\n",
    "                        fail_count == 0,\n",
    "                        f\"{fail_count} nulls\",\n",
    "                        success_count,\n",
    "                        fail_count,\n",
    "                        bad_payload))\n",
    "\n",
    "    # 3. Schema Checks\n",
    "    actual_fields = {f.name: f.dataType for f in df.schema.fields}\n",
    "    for field in expected_schema.fields:\n",
    "        actual_type = actual_fields.get(field.name)\n",
    "        dq_logs.append((datetime.utcnow(), job_name, trade_date_val,\n",
    "                        f\"SchemaCheck_{field.name}\",\n",
    "                        actual_type == field.dataType,\n",
    "                        str(actual_type),\n",
    "                        total_count if actual_type == field.dataType else 0,\n",
    "                        0 if actual_type == field.dataType else total_count,\n",
    "                        None))\n",
    "\n",
    "    # Convert to DF\n",
    "    #dq_df = spark.createDataFrame(\n",
    "    #    dq_logs,\n",
    "    #    [\"validation_time\", \"job_name\", \"trade_date\", \"rule_name\",\n",
    "    #     \"status\", \"detail\", \"success_count\", \"failure_count\", \"bad_record\"]\n",
    "    #).withColumn(\"trade_date\", to_date(col(\"trade_date\")))\n",
    "    \n",
    "    dq_df = spark.createDataFrame(dq_logs, schema=dq_log_schema)\n",
    "\n",
    "\n",
    "    # Append to table\n",
    "    dq_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"monitoring.data_quality_log\")\n",
    "\n",
    "    return dq_df\n",
    "\n",
    "# Run validation and log results\n",
    "dq_log_df = validate_and_log(df_incremental, expected_schema, job_name=\"btc_usd_pipeline\")\n",
    "\n",
    "# Show results inline\n",
    "dq_log_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f8361e1-306a-4d53-b545-9499e172c7c3",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"rule_name\":235},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758456819747}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>trade_date</th><th>rule_name</th><th>total_failures</th></tr></thead><tbody><tr><td>2025-09-21</td><td>SchemaCheck_trade_id</td><td>0</td></tr><tr><td>2025-09-21</td><td>SchemaCheck_side</td><td>0</td></tr><tr><td>2025-09-21</td><td>SchemaCheck_price</td><td>0</td></tr><tr><td>2025-09-21</td><td>NullCheck_trade_id</td><td>0</td></tr><tr><td>2025-09-21</td><td>SchemaCheck_trade_date</td><td>1098</td></tr><tr><td>2025-09-21</td><td>NullCheck_ingest_ts</td><td>0</td></tr><tr><td>2025-09-21</td><td>NullCheck_side</td><td>0</td></tr><tr><td>2025-09-21</td><td>SchemaCheck_ingest_ts</td><td>1098</td></tr><tr><td>2025-09-21</td><td>RowCount</td><td>0</td></tr><tr><td>2025-09-21</td><td>NullCheck_trade_time</td><td>0</td></tr><tr><td>2025-09-21</td><td>NullCheck_size</td><td>0</td></tr><tr><td>2025-09-21</td><td>NullCheck_price</td><td>0</td></tr><tr><td>2025-09-21</td><td>NullCheck_trade_date</td><td>0</td></tr><tr><td>2025-09-21</td><td>SchemaCheck_size</td><td>0</td></tr><tr><td>2025-09-21</td><td>SchemaCheck_trade_time</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2025-09-21",
         "SchemaCheck_trade_id",
         0
        ],
        [
         "2025-09-21",
         "SchemaCheck_side",
         0
        ],
        [
         "2025-09-21",
         "SchemaCheck_price",
         0
        ],
        [
         "2025-09-21",
         "NullCheck_trade_id",
         0
        ],
        [
         "2025-09-21",
         "SchemaCheck_trade_date",
         1098
        ],
        [
         "2025-09-21",
         "NullCheck_ingest_ts",
         0
        ],
        [
         "2025-09-21",
         "NullCheck_side",
         0
        ],
        [
         "2025-09-21",
         "SchemaCheck_ingest_ts",
         1098
        ],
        [
         "2025-09-21",
         "RowCount",
         0
        ],
        [
         "2025-09-21",
         "NullCheck_trade_time",
         0
        ],
        [
         "2025-09-21",
         "NullCheck_size",
         0
        ],
        [
         "2025-09-21",
         "NullCheck_price",
         0
        ],
        [
         "2025-09-21",
         "NullCheck_trade_date",
         0
        ],
        [
         "2025-09-21",
         "SchemaCheck_size",
         0
        ],
        [
         "2025-09-21",
         "SchemaCheck_trade_time",
         0
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "trade_date",
            "nullable": true,
            "type": "date"
           },
           {
            "metadata": {},
            "name": "rule_name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "total_failures",
            "nullable": true,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 71
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "trade_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "rule_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_failures",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT trade_date, rule_name, SUM(failure_count) AS total_failures\n",
    "FROM monitoring.data_quality_log\n",
    "WHERE trade_date = current_date()\n",
    "GROUP BY trade_date, rule_name;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e5f6782-b9f8-43c8-b2e8-6add51dd7585",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"rule_name\":221},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758459497691}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>trade_date</th><th>rule_name</th><th>passed</th><th>failed</th><th>pass_percent</th></tr></thead><tbody><tr><td>2025-09-21</td><td>SchemaCheck_trade_id</td><td>1098</td><td>0</td><td>100.00</td></tr><tr><td>2025-09-21</td><td>SchemaCheck_side</td><td>1098</td><td>0</td><td>100.00</td></tr><tr><td>2025-09-21</td><td>SchemaCheck_price</td><td>1098</td><td>0</td><td>100.00</td></tr><tr><td>2025-09-21</td><td>NullCheck_trade_id</td><td>1098</td><td>0</td><td>100.00</td></tr><tr><td>2025-09-21</td><td>SchemaCheck_trade_date</td><td>0</td><td>1098</td><td>0.00</td></tr><tr><td>2025-09-21</td><td>NullCheck_ingest_ts</td><td>1098</td><td>0</td><td>100.00</td></tr><tr><td>2025-09-21</td><td>NullCheck_side</td><td>1098</td><td>0</td><td>100.00</td></tr><tr><td>2025-09-21</td><td>SchemaCheck_ingest_ts</td><td>0</td><td>1098</td><td>0.00</td></tr><tr><td>2025-09-21</td><td>RowCount</td><td>1098</td><td>0</td><td>100.00</td></tr><tr><td>2025-09-21</td><td>NullCheck_trade_time</td><td>1098</td><td>0</td><td>100.00</td></tr><tr><td>2025-09-21</td><td>NullCheck_size</td><td>1098</td><td>0</td><td>100.00</td></tr><tr><td>2025-09-21</td><td>NullCheck_price</td><td>1098</td><td>0</td><td>100.00</td></tr><tr><td>2025-09-21</td><td>NullCheck_trade_date</td><td>1098</td><td>0</td><td>100.00</td></tr><tr><td>2025-09-21</td><td>SchemaCheck_size</td><td>1098</td><td>0</td><td>100.00</td></tr><tr><td>2025-09-21</td><td>SchemaCheck_trade_time</td><td>1098</td><td>0</td><td>100.00</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2025-09-21",
         "SchemaCheck_trade_id",
         1098,
         0,
         "100.00"
        ],
        [
         "2025-09-21",
         "SchemaCheck_side",
         1098,
         0,
         "100.00"
        ],
        [
         "2025-09-21",
         "SchemaCheck_price",
         1098,
         0,
         "100.00"
        ],
        [
         "2025-09-21",
         "NullCheck_trade_id",
         1098,
         0,
         "100.00"
        ],
        [
         "2025-09-21",
         "SchemaCheck_trade_date",
         0,
         1098,
         "0.00"
        ],
        [
         "2025-09-21",
         "NullCheck_ingest_ts",
         1098,
         0,
         "100.00"
        ],
        [
         "2025-09-21",
         "NullCheck_side",
         1098,
         0,
         "100.00"
        ],
        [
         "2025-09-21",
         "SchemaCheck_ingest_ts",
         0,
         1098,
         "0.00"
        ],
        [
         "2025-09-21",
         "RowCount",
         1098,
         0,
         "100.00"
        ],
        [
         "2025-09-21",
         "NullCheck_trade_time",
         1098,
         0,
         "100.00"
        ],
        [
         "2025-09-21",
         "NullCheck_size",
         1098,
         0,
         "100.00"
        ],
        [
         "2025-09-21",
         "NullCheck_price",
         1098,
         0,
         "100.00"
        ],
        [
         "2025-09-21",
         "NullCheck_trade_date",
         1098,
         0,
         "100.00"
        ],
        [
         "2025-09-21",
         "SchemaCheck_size",
         1098,
         0,
         "100.00"
        ],
        [
         "2025-09-21",
         "SchemaCheck_trade_time",
         1098,
         0,
         "100.00"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "trade_date",
            "nullable": true,
            "type": "date"
           },
           {
            "metadata": {},
            "name": "rule_name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "passed",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "failed",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "pass_percent",
            "nullable": true,
            "type": "decimal(27,2)"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 72
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "trade_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "rule_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "passed",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "failed",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "pass_percent",
         "type": "\"decimal(27,2)\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT trade_date, \n",
    "       rule_name,\n",
    "       SUM(success_count) AS passed,\n",
    "       SUM(failure_count) AS failed,\n",
    "       ROUND(SUM(success_count)*100.0 / (SUM(success_count)+SUM(failure_count)), 2) AS pass_percent\n",
    "FROM monitoring.data_quality_log\n",
    "GROUP BY trade_date, rule_name;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d0c46a1-3493-447d-b750-80b745c515cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>validation_time</th><th>job_name</th><th>trade_date</th><th>rule_name</th><th>status</th><th>detail</th><th>success_count</th><th>failure_count</th><th>bad_record</th></tr></thead><tbody><tr><td>2025-09-21T12:57:13.372391Z</td><td>btc_usd_pipeline</td><td>2025-09-21</td><td>SchemaCheck_trade_date</td><td>false</td><td>StringType()</td><td>0</td><td>1098</td><td>null</td></tr><tr><td>2025-09-21T12:57:13.372425Z</td><td>btc_usd_pipeline</td><td>2025-09-21</td><td>SchemaCheck_ingest_ts</td><td>false</td><td>StringType()</td><td>0</td><td>1098</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2025-09-21T12:57:13.372391Z",
         "btc_usd_pipeline",
         "2025-09-21",
         "SchemaCheck_trade_date",
         false,
         "StringType()",
         0,
         1098,
         null
        ],
        [
         "2025-09-21T12:57:13.372425Z",
         "btc_usd_pipeline",
         "2025-09-21",
         "SchemaCheck_ingest_ts",
         false,
         "StringType()",
         0,
         1098,
         null
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "validation_time",
            "nullable": true,
            "type": "timestamp"
           },
           {
            "metadata": {},
            "name": "job_name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "trade_date",
            "nullable": true,
            "type": "date"
           },
           {
            "metadata": {},
            "name": "rule_name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {
             "comment": "TRUE if all passed, FALSE if any failed"
            },
            "name": "status",
            "nullable": true,
            "type": "boolean"
           },
           {
            "metadata": {
             "comment": "Extra info like datatype or row count"
            },
            "name": "detail",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {
             "comment": "Number of records that passed this rule"
            },
            "name": "success_count",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {
             "comment": "Number of records that failed this rule"
            },
            "name": "failure_count",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {
             "comment": "Sample JSON of failing records (optional)"
            },
            "name": "bad_record",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 62
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "validation_time",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "job_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "trade_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "rule_name",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"TRUE if all passed, FALSE if any failed\"}",
         "name": "status",
         "type": "\"boolean\""
        },
        {
         "metadata": "{\"comment\":\"Extra info like datatype or row count\"}",
         "name": "detail",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"Number of records that passed this rule\"}",
         "name": "success_count",
         "type": "\"long\""
        },
        {
         "metadata": "{\"comment\":\"Number of records that failed this rule\"}",
         "name": "failure_count",
         "type": "\"long\""
        },
        {
         "metadata": "{\"comment\":\"Sample JSON of failing records (optional)\"}",
         "name": "bad_record",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "SELECT *\n",
    "FROM monitoring.data_quality_log\n",
    "where rule_name in ('SchemaCheck_trade_date','SchemaCheck_ingest_ts')\n",
    "\n",
    "/*\n",
    "For rules like SchemaCheck_trade_date and SchemaCheck_ingest_ts, the check is comparing expected type vs actual type.\n",
    "If they don’t match, you’re marking every row as “failed” (failure_count = total_count).\n",
    "But since this isn’t a row-level failure (it’s a schema mismatch), there’s no “bad record” subset to log — hence bad_record = NULL.\n",
    "*/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6230574895729538,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DQ-Validation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}