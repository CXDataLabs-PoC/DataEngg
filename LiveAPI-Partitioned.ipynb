{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f63961e1-599b-4865-ac40-4b35c4da85e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "STORAGE_ACCOUNT = \"cxdlbbronze\"\n",
    "CONTAINER = \"landingzone\"\n",
    "Folder = \"BTC-USD-Partitioned\"\n",
    "\n",
    "lakehouse_path = f\"abfss://{CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net/{Folder}\"\n",
    "checkpoint_path = lakehouse_path + \"_checkpoint/last_trade.json\"\n",
    "\n",
    "client_id = dbutils.secrets.get(scope=\"landingzone-secret\", key=\"client-id\")\n",
    "client_secret = dbutils.secrets.get(scope=\"landingzone-secret\", key=\"client-secret\")\n",
    "tenant_id = dbutils.secrets.get(scope=\"landingzone-secret\", key=\"tenant-id\")\n",
    "\n",
    "account_host = f\"{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{account_host}\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{account_host}\",\n",
    "               \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{account_host}\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{account_host}\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{account_host}\",\n",
    "               f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b34d5708-f9c1-413c-a9a2-7347ffaba146",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.rm(\"abfss://landingzone@cxdlbbronze.dfs.core.windows.net/BTC-USD-Partitioned/\", recurse=True)\n",
    "dbutils.fs.rm(\"abfss://landingzone@cxdlbbronze.dfs.core.windows.net/BTC-USD-Partitioned_checkpoint/\", recurse=True)\n",
    "dbutils.fs.rm(\"abfss://landingzone@cxdlbbronze.dfs.core.windows.net/BTC-USD-Partitioned_checkpoint/\", recurse=True)\n",
    "dbutils.fs.rm(\"abfss://landingzone@cxdlbbronze.dfs.core.windows.net/BTC-USD-Partitioned_parquet/\", recurse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "344c127e-40ca-4304-a9d6-45a0f17f41fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last processed trade_id = 876394297\n+---------------+----+----------+---------------------------+---------+\n|price          |side|size      |time                       |trade_id |\n+---------------+----+----------+---------------------------+---------+\n|115704.39000000|sell|0.00021391|2025-09-21T11:18:50.317826Z|876394379|\n|115704.39000000|sell|0.00004064|2025-09-21T11:18:47.374875Z|876394378|\n|115704.39000000|sell|0.00004098|2025-09-21T11:18:46.539722Z|876394377|\n|115704.38000000|buy |0.00013658|2025-09-21T11:18:43.976789Z|876394376|\n|115704.39000000|sell|0.00020997|2025-09-21T11:18:43.200237Z|876394375|\n+---------------+----+----------+---------------------------+---------+\nonly showing top 5 rows\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.ipykernel/1387/command-7967540249864300-4294910994:97: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  .withColumn(\"ingest_ts\", lit(datetime.utcnow()))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+--------+\n|Test          |Duration_sec      |RowCount|\n+--------------+------------------+--------+\n|Delta Count   |0.7224810123443604|1098    |\n|Parquet Count |0.7677757740020752|1098    |\n|Delta Filter  |0.8253052234649658|1098    |\n|Parquet Filter|0.6873869895935059|1098    |\n+--------------+------------------+--------+\n\n✅ Loaded 82 new trades. Updated watermark = 876394379\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, LongType, TimestampType, DecimalType, StringType\n",
    "from pyspark.sql.functions import col, lit, max as spark_max, to_timestamp, date_format\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "api_url = \"https://api.exchange.coinbase.com/products/BTC-USD/trades\"\n",
    "\n",
    "\n",
    "STORAGE_ACCOUNT = \"cxdlbbronze\"\n",
    "CONTAINER = \"landingzone\"\n",
    "Folder = \"BTC-USD-Partitioned\"\n",
    "\n",
    "lakehouse_path = f\"abfss://{CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net/{Folder}\"\n",
    "checkpoint_path = lakehouse_path + \"_checkpoint/last_trade.json\"\n",
    "\n",
    "parquet_path = lakehouse_path + \"_parquet\"\n",
    "\n",
    "client_id = dbutils.secrets.get(scope=\"landingzone-secret\", key=\"client-id\")\n",
    "client_secret = dbutils.secrets.get(scope=\"landingzone-secret\", key=\"client-secret\")\n",
    "tenant_id = dbutils.secrets.get(scope=\"landingzone-secret\", key=\"tenant-id\")\n",
    "\n",
    "account_host = f\"{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{account_host}\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{account_host}\",\n",
    "               \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{account_host}\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{account_host}\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{account_host}\",\n",
    "               f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "#By default Spark uses 200 shuffle partitions — too high\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 8)\n",
    "\n",
    "\n",
    "def benchmark(name, action):\n",
    "    start = time.time()\n",
    "    result = action()\n",
    "    duration = time.time() - start\n",
    "    return (name, duration, result if isinstance(result, int) else None)\n",
    "\n",
    "# -------------------------\n",
    "# Get last watermark (trade_id)\n",
    "# -------------------------\n",
    "def get_last_trade_id():\n",
    "    try:\n",
    "        df = spark.read.json(checkpoint_path)\n",
    "        return df.collect()[0][\"last_trade_id\"]\n",
    "    except Exception:\n",
    "        return 0   # start from 0 for first run\n",
    "\n",
    "last_trade_id = get_last_trade_id()\n",
    "print(f\"Last processed trade_id = {last_trade_id}\")\n",
    "\n",
    "# -------------------------\n",
    "# Define standardized schema\n",
    "# -------------------------\n",
    "trade_schema = StructType([\n",
    "    StructField(\"trade_id\", LongType(), False),\n",
    "    StructField(\"trade_time\", TimestampType(), False),\n",
    "    StructField(\"price\", DecimalType(18,2), False),\n",
    "    StructField(\"size\", DecimalType(18,8), False),\n",
    "    StructField(\"side\", StringType(), False),\n",
    "    StructField(\"ingest_ts\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "# -------------------------\n",
    "# Fetch trades from Coinbase API\n",
    "# -------------------------\n",
    "response = requests.get(api_url, headers={\"Accept\": \"application/json\"})\n",
    "if response.status_code != 200:\n",
    "    raise Exception(f\"API call failed: {response.text}\")\n",
    "\n",
    "data = response.json()\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "raw_df = spark.read.json(spark.sparkContext.parallelize([json.dumps(d) for d in data]))\n",
    "\n",
    "# Coinbase API returns fields: time, trade_id, price, size, side\n",
    "raw_df.show(5, truncate=False)\n",
    "\n",
    "# -------------------------\n",
    "# Transform + Standardize schema\n",
    "# -------------------------\n",
    "df = (raw_df\n",
    "    .withColumn(\"trade_id\", col(\"trade_id\").cast(LongType()))\n",
    "    .withColumn(\"trade_time\", to_timestamp(col(\"time\")))\n",
    "    .withColumn(\"price\", col(\"price\").cast(DecimalType(18,2)))\n",
    "    .withColumn(\"size\", col(\"size\").cast(DecimalType(18,8)))\n",
    "    .withColumn(\"ingest_ts\", lit(datetime.utcnow()))\n",
    "    .withColumn(\"trade_date\", date_format(col(\"trade_time\"), \"yyyy-MM-dd\"))  # string date\n",
    "    .select(\"trade_id\", \"trade_time\", \"price\", \"size\", \"side\", \"ingest_ts\",\"trade_date\")\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Filter Incremental Records\n",
    "# -------------------------\n",
    "df_incremental = df.filter(col(\"trade_id\") > last_trade_id)\n",
    "\n",
    "if df_incremental.count() == 0:\n",
    "    print(\"No new trades to load.\")\n",
    "else:\n",
    "    # Add load timestamp for auditing\n",
    "    df_incremental = df_incremental.withColumn(\"ingest_ts\", lit(datetime.now().isoformat()))\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Write to Azure Data Lakehouse (Delta format)\n",
    "    # ---------------------------------------------\n",
    "    df_incremental.write.format(\"delta\").partitionBy(\"trade_date\").mode(\"append\").save(lakehouse_path)\n",
    "\n",
    "    # Write as Parquet\n",
    "    df_incremental.write.mode(\"append\").parquet(parquet_path)\n",
    "    benchmarks = []\n",
    "    # Count trades\n",
    "    benchmarks.append(benchmark(\"Delta Count\", lambda: spark.read.format(\"delta\").load(lakehouse_path).count()))\n",
    "    benchmarks.append(benchmark(\"Parquet Count\", lambda: spark.read.parquet(parquet_path).count()))\n",
    "\n",
    "    # Partition filter\n",
    "    benchmarks.append(benchmark(\"Delta Filter\", lambda: spark.read.format(\"delta\").load(lakehouse_path).filter(\"trade_date='2025-09-21'\").count()))\n",
    "    benchmarks.append(benchmark(\"Parquet Filter\", lambda: spark.read.parquet(parquet_path).filter(\"trade_date='2025-09-21'\").count()))\n",
    "    bench_df = spark.createDataFrame(benchmarks, [\"Test\", \"Duration_sec\", \"RowCount\"])\n",
    "    bench_df.show(truncate=False)\n",
    "\n",
    "    # -------------------------\n",
    "    # Update watermark\n",
    "    # -------------------------\n",
    "    new_trade_id = df_incremental.agg(spark_max(\"trade_id\")).collect()[0][0]\n",
    "    spark.createDataFrame([{\"last_trade_id\": new_trade_id}]).write.mode(\"append\").json(checkpoint_path)\n",
    "\n",
    "    print(f\"✅ Loaded {df_incremental.count()} new trades. Updated watermark = {new_trade_id}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LiveAPI-Partitioned",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}